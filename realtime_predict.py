import cv2
import torch
import numpy as np
import time
from collections import deque
from gesture_dataset.model import GestureCNNBiLSTM
import mediapipe as mp

# ì„¤ì •
CONFIDENCE_THRESHOLD = 0.6  # softmax í™•ì‹ ë„ ê¸°ì¤€
STABLE_THRESHOLD = 3        # ê°™ì€ ì˜ˆì¸¡ ëª‡ ë²ˆ ì—°ì†ì´ë©´ í™•ì •
HOLD_TIME = 2.0             # í™•ì • í›„ ìœ ì§€ ì‹œê°„ (ì´ˆ)
COOLDOWN_TIME = 1.5         # ë‹¤ìŒ ì œìŠ¤ì²˜ ì˜ˆì¸¡ê¹Œì§€ ëŒ€ê¸° ì‹œê°„

# ëª¨ë¸
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = GestureCNNBiLSTM(input_dim=63, cnn_out=128, lstm_hidden=128, output_dim=15).to(device)
model.load_state_dict(torch.load("gesture_cnn_bilstm.pt", map_location=device))
model.eval()

# MediaPipe
mp_hands = mp.solutions.hands
hands = mp_hands.Hands(static_image_mode=False, max_num_hands=1)
mp_drawing = mp.solutions.drawing_utils

# ë²„í¼ & ë¼ë²¨
seq_length = 10
buffer = deque(maxlen=seq_length)

label_map = {
    0: "Unknown",
    1: "D0X (None)",
    2: "B0A (pointing with one finger)",
    3: "B0B (pointing with two fingers)",
    4: "G01 (Click with one finger)",
    5: "G02 (Click with two fingers)",
    6: "G03 (Throw up)",
    7: "G04 (Throw down)",
    8: "G05 (Throw left)",
    9: "G06 (Throw right)",
    10: "G07 (Open twice)",
    11: "G08 (Double click with one finger)",
    12: "G09 (Double click with two fingers)",
    13: "G10 (Zoom in)",
    14: "G11 (Zoom out)"
}

# ìƒíƒœ ë³€ìˆ˜
last_pred, stable_pred = None, None
pred_count = 0
last_confirm_time = time.time()
last_output_time = 0

cap = cv2.VideoCapture(0)
while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break

    image = cv2.flip(frame, 1)
    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    result = hands.process(image_rgb)

    if result.multi_hand_landmarks:
        lm = result.multi_hand_landmarks[0]
        coords = [(p.x, p.y, p.z) for p in lm.landmark]
        buffer.append(np.array(coords).flatten())
        mp_drawing.draw_landmarks(image, lm, mp_hands.HAND_CONNECTIONS)

        # ì˜ˆì¸¡ ì²˜ë¦¬
        if len(buffer) == seq_length:
            input_tensor = torch.tensor([buffer], dtype=torch.float32).to(device)
            with torch.no_grad():
                logits = model(input_tensor)
                probs = torch.softmax(logits, dim=1)
                conf, pred = torch.max(probs, dim=1)
                pred = pred.item()
                conf = conf.item()

                now = time.time()

                # í™•ì‹ ë„ ë‚®ìœ¼ë©´ ë¬´ì‹œ
                if conf < CONFIDENCE_THRESHOLD:
                    pred = -1  # invalid prediction

                # ì˜ˆì¸¡ ì•ˆì •í™” ì²˜ë¦¬
                if pred == last_pred:
                    pred_count += 1
                else:
                    pred_count = 1
                    last_pred = pred

                # ì˜ˆì¸¡ì´ ì•ˆì •ëê³  cooldown ì§€ë‚¬ê³  D0Xê°€ ì•„ë‹ ë•Œë§Œ í™•ì •
                if (
                    pred_count >= STABLE_THRESHOLD and
                    pred != 1 and pred != -1 and
                    (now - last_output_time) > COOLDOWN_TIME
                ):
                    stable_pred = pred
                    last_confirm_time = now
                    last_output_time = now

                    confirmed_gesture = label_map.get(pred, "Unknown")
                    print(f"âœ… ì¸ì‹ í™•ì •: {confirmed_gesture}")

                    # í–‰ë™ ì˜ˆì‹œ
                    if confirmed_gesture.startswith("G01"):
                        print("ğŸŸ¢ ë¶ˆ ì¼œê¸°")
                    else:
                        print(f"â¡ï¸ ì œìŠ¤ì²˜: {confirmed_gesture}")

    # í™”ë©´ ì¶œë ¥ ì²˜ë¦¬
    if stable_pred is not None and (time.time() - last_confirm_time < HOLD_TIME):
        display_text = label_map.get(stable_pred, "...")
    else:
        display_text = "..."

    cv2.putText(image, display_text, (10, 30),
                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

    cv2.imshow("SIGNAL - ì‹¤ì‹œê°„ ì œìŠ¤ì²˜ ì¸ì‹ (ê°•ê±´ ëª¨ë“œ)", image)
    if cv2.waitKey(1) & 0xFF == 27:
        break

cap.release()
cv2.destroyAllWindows()
