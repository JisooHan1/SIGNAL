# realtime_predict.py

import cv2
import torch
import numpy as np
from collections import deque
from gesture_dataset.model import GestureBiLSTM
import mediapipe as mp

# ëª¨ë¸ ë¡œë“œ
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = GestureBiLSTM(input_dim=63, hidden_dim=128, output_dim=14).to(device)
model.load_state_dict(torch.load("gesture_bilstm.pt", map_location=device))
model.eval()

# MediaPipe ì¤€ë¹„
mp_hands = mp.solutions.hands
hands = mp_hands.Hands(static_image_mode=False, max_num_hands=1)
mp_drawing = mp.solutions.drawing_utils

# ì‹¤ì‹œê°„ ì…ë ¥ ë²„í¼ (ìµœê·¼ 10í”„ë ˆì„)
seq_length = 10
buffer = deque(maxlen=seq_length)

# ë¼ë²¨ ë§¤í•‘ (ì„ì‹œ ì˜ˆì‹œ: ì›í•˜ëŠ” ì¶œë ¥ìœ¼ë¡œ ìˆ˜ì • ê°€ëŠ¥)
label_map = {
    0: "D0X (None)",
    1: "B0A (One finger)",
    2: "B0B (Two fingers)",
    3: "G01 (Click)",
    4: "G02",
    5: "G03",
    6: "G04",
    7: "G05",
    8: "G06",
    9: "G07",
    10: "G08",
    11: "G09",
    12: "G10 (Zoom in)",
    13: "G11 (Zoom out)"
}

# ì›¹ìº  ì‹œì‘
cap = cv2.VideoCapture(0)
while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break

    image = cv2.flip(frame, 1)
    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    result = hands.process(image_rgb)

    if result.multi_hand_landmarks:
        lm = result.multi_hand_landmarks[0]
        coords = [(p.x, p.y, p.z) for p in lm.landmark]
        buffer.append(np.array(coords).flatten())
        print("âœ… Buffer length:", len(buffer))
        print("ğŸ” First landmark x,y,z:", coords[0])  # ì²« ë²ˆì§¸ ê´€ì ˆ ìœ„ì¹˜
        print("ğŸ“Š Mean Z of hand:", np.mean([p.z for p in lm.landmark]))


        mp_drawing.draw_landmarks(image, lm, mp_hands.HAND_CONNECTIONS)

        # ì˜ˆì¸¡
        if len(buffer) == seq_length:
            input_tensor = torch.tensor([buffer], dtype=torch.float32).to(device)
            with torch.no_grad():
                output = model(input_tensor)
                pred = torch.argmax(output, dim=1).item()
                gesture = label_map.get(pred, "Unknown")

                # ğŸ”¥ ì—¬ê¸°ì„œ ì œì–´ ì¶œë ¥ (ì‹¤ì œ ì œì–´ëŠ” send_command ë“±ìœ¼ë¡œ ëŒ€ì²´)
                if gesture.startswith("G01"):
                    print("ğŸŸ¢ ë¶ˆ ì¼œê¸° (Click)")
                elif gesture.startswith("D0X"):
                    print("âšª ì•„ë¬´ ë™ì‘ ì•„ë‹˜")
                else:
                    print(f"â¡ï¸ ì¸ì‹ëœ ì œìŠ¤ì²˜: {gesture}")

                # í™”ë©´ì— ì¶œë ¥
                cv2.putText(image, gesture, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

    cv2.imshow("SIGNAL - ì‹¤ì‹œê°„ ì œìŠ¤ì²˜ ì¸ì‹", image)
    if cv2.waitKey(1) & 0xFF == 27:
        break

cap.release()
cv2.destroyAllWindows()
